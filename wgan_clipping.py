# -*- coding: utf-8 -*-
"""WGAN clipping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16aVvqUi8Nq4PtcYC-pRaguUztoeKfwmM
"""

import numpy as np
import matplotlib.pyplot as plt
from torchvision import datasets, transforms

# load MNIST
train_data = datasets.MNIST(root='./data', train=True, download=True,
                                    transform=transforms.ToTensor())
train_images = [img.numpy().reshape(-1) for img, _ in train_data]
train_images = np.stack(train_images, axis=0).T  # Shape: (784, 60000)

input_size = 100
hidden_size1 = 128
hidden_size2 = 256
hidden_size3 = 512
image_size = 784

D_lr = 0.005
G_lr = 0.005
epochs = 22000
batch_size = 64
D_epoch = 3
clip_value = 0.05

def sigmoid(x): return 1 / (1 + np.exp(-x) + 1e-8)  # plus 1e-8 to prevent exploding gradients
def leaky_relu(x, alpha=0.2): return np.maximum(alpha * x, x)
def leaky_relu_deriv(x, alpha=0.2): return np.where(x > 0, 1, alpha)

# xavier initialization
def xavier_init(shape):
    fan_in = shape[1]
    std = np.sqrt(2.0 / fan_in)
    return np.random.randn(*shape) * std

G_W1 = xavier_init((hidden_size1, input_size))
G_b1 = np.zeros((hidden_size1, 1))
G_W2 = xavier_init((hidden_size2, hidden_size1))
G_b2 = np.zeros((hidden_size2, 1))
G_W3 = xavier_init((hidden_size3, hidden_size2))
G_b3 = np.zeros((hidden_size3, 1))
G_W4 = xavier_init((image_size, hidden_size3))
G_b4 = np.zeros((image_size, 1))

D_W1 = xavier_init((hidden_size3, image_size))
D_b1 = np.zeros((hidden_size3, 1))
D_W2 = xavier_init((hidden_size2, hidden_size3))
D_b2 = np.zeros((hidden_size2, 1))
D_W3 = xavier_init((hidden_size1, hidden_size2))
D_b3 = np.zeros((hidden_size1, 1))
D_W4 = xavier_init((1, hidden_size1))
D_b4 = np.zeros((1, 1))

def generator(z):
    a1 = leaky_relu(np.dot(G_W1, z) + G_b1)
    a2 = leaky_relu(np.dot(G_W2, a1) + G_b2)
    a3 = leaky_relu(np.dot(G_W3, a2) + G_b3)
    x = sigmoid(np.dot(G_W4, a3) + G_b4)
    return x, a1, a2, a3

def discriminator(x):
    a1 = leaky_relu(np.dot(D_W1, x) + D_b1)
    a2 = leaky_relu(np.dot(D_W2, a1) + D_b2)
    a3 = leaky_relu(np.dot(D_W3, a2) + D_b3)
    y = np.dot(D_W4, a3) + D_b4  # no sigmoid for WGAN
    return y, a1, a2, a3

D_losses = []
G_losses = []

for epoch in range(epochs):
    for _ in range(D_epoch):
        idx = np.random.choice(train_images.shape[1], batch_size, replace=False)
        real_images = train_images[:, idx]  # get a batch of real images (64 images per batch)
        z = np.random.randn(input_size, batch_size)  # random noise for generator
        fake_images, _, _, _ = generator(z)

        D_real_pred, D_real_h1, D_real_h2, D_real_h3 = discriminator(real_images)
        D_fake_pred, D_fake_h1, D_fake_h2, D_fake_h3 = discriminator(fake_images)

        # Wasserstein loss for D
        # J_D = E_x~p_data[D(x)] - E_x~p_g[D(x)] = E_x~p_data[f_theta(x)] - E_x~p_g[f_theta(x)]
        # applying the law of large numbers: approximating the expected value by the sample mean
        D_loss = -np.mean(D_real_pred) + np.mean(D_fake_pred)

        # gradients for h^[4]
        dD_real = -np.ones((1, batch_size)) / batch_size  # ∂J_D/∂h_real^[4]
        dD_fake = np.ones((1, batch_size)) / batch_size  # ∂J_D/∂h_fake^[4]

        # gradients for W^[4] and b^[4]
        dD_W4 = dD_real @ D_real_h3.T + dD_fake @ D_fake_h3.T
        # ∂J_D/∂W^[4] = B[MM, W](∂J_D/∂h_real^[4]) + B[MM, W](∂J_D/∂h_fake^[4])
        dD_b4 = np.sum(dD_real + dD_fake, axis=1, keepdims=True)
        # ∂J_D/∂b^[4] = B[MM, b](∂J_D/∂h_real^[4]) + B[MM, b](∂J_D/∂h_fake^[4])

        # gradients for h^[3]
        dD_h3_real = D_W4.T @ dD_real  # ∂J_D/∂a_real^[3] = B[MM, a](∂J_D/∂h_real^[4])
        dD_h3_real *= leaky_relu_deriv(D_real_h3)  # ∂J_D/∂h_real^[3] = B[σ, h](∂J_D/∂a_real^[3])

        dD_h3_fake = D_W4.T @ dD_fake  # ∂J_D/∂a_fake^[3] = B[MM, a](∂J_D/∂h_fake^[4])
        dD_h3_fake *= leaky_relu_deriv(D_fake_h3)  # ∂J_D/∂h_fake^[3] = B[σ, h](∂J_D/∂a_fake^[3])

        # gradients for W^[3] and b^[3]
        dD_W3 = dD_h3_real @ D_real_h2.T + dD_h3_fake @ D_fake_h2.T
        dD_b3 = np.sum(dD_h3_real + dD_h3_fake, axis=1, keepdims=True)

        # continue backpropagation through previous layers...
        dD_h2_real = D_W3.T @ dD_h3_real
        dD_h2_real *= leaky_relu_deriv(D_real_h2)

        dD_h2_fake = D_W3.T @ dD_h3_fake
        dD_h2_fake *= leaky_relu_deriv(D_fake_h2)


        dD_W2 = dD_h2_real @ D_real_h1.T + dD_h2_fake @ D_fake_h1.T
        dD_b2 = np.sum(dD_h2_real + dD_h2_fake, axis=1, keepdims=True)


        dD_h1_real = D_W2.T @ dD_h2_real
        dD_h1_real *= leaky_relu_deriv(D_real_h1)

        dD_h1_fake = D_W2.T @ dD_h2_fake
        dD_h1_fake *= leaky_relu_deriv(D_fake_h1)

        dD_W1 = dD_h1_real @ real_images.T + dD_h1_fake @ fake_images.T
        dD_b1 = np.sum(dD_h1_real + dD_h1_fake, axis=1, keepdims=True)

        # gradient descent of D
        D_W4 -= D_lr * dD_W4
        D_b4 -= D_lr * dD_b4
        D_W3 -= D_lr * dD_W3
        D_b3 -= D_lr * dD_b3
        D_W2 -= D_lr * dD_W2
        D_b2 -= D_lr * dD_b2
        D_W1 -= D_lr * dD_W1
        D_b1 -= D_lr * dD_b1

        # clip weight to satisfy Lipschitz continuity (1-Lipschitz)
        D_W1 = np.clip(D_W1, -clip_value, clip_value)
        D_W2 = np.clip(D_W2, -clip_value, clip_value)
        D_W3 = np.clip(D_W3, -clip_value, clip_value)
        D_W4 = np.clip(D_W4, -clip_value, clip_value)
        D_b1 = np.clip(D_b1, -clip_value, clip_value)
        D_b2 = np.clip(D_b2, -clip_value, clip_value)
        D_b3 = np.clip(D_b3, -clip_value, clip_value)
        D_b4 = np.clip(D_b4, -clip_value, clip_value)

    z = np.random.randn(input_size, batch_size)
    fake_images, G_h1, G_h2, G_h3 = generator(z)
    D_fake_pred, D_fake_h1, D_fake_h2, D_fake_h3 = discriminator(fake_images)

    # Wasserstein loss for G
    G_loss = -np.mean(D_fake_pred)  # J_G = -E_x~p_g[D(x)] = -E_x~p_g[f_theta(x)]

    dD_output = -np.ones((1, batch_size)) / batch_size  # ∂J_G/∂h^[4]

    dD_h3 = D_W4.T @ dD_output
    dD_h3 *= leaky_relu_deriv(D_fake_h3)

    dD_h2 = D_W3.T @ dD_h3
    dD_h2 *= leaky_relu_deriv(D_fake_h2)

    dD_h1 = D_W2.T @ dD_h2
    dD_h1 *= leaky_relu_deriv(D_fake_h1)

    dG_output = D_W1.T @ dD_h1
    dG_output *= fake_images * (1 - fake_images)

    dG_W4 = dG_output @ G_h3.T
    dG_b4 = np.sum(dG_output, axis=1, keepdims=True)

    dG_h3 = G_W4.T @ dG_output
    dG_h3 *= leaky_relu_deriv(G_h3)

    dG_W3 = dG_h3 @ G_h2.T
    dG_b3 = np.sum(dG_h3, axis=1, keepdims=True)

    dG_h2 = G_W3.T @ dG_h3
    dG_h2 *= leaky_relu_deriv(G_h2)

    dG_W2 = dG_h2 @ G_h1.T
    dG_b2 = np.sum(dG_h2, axis=1, keepdims=True)

    dG_h1 = G_W2.T @ dG_h2
    dG_h1 *= leaky_relu_deriv(G_h1)

    dG_W1 = dG_h1 @ z.T
    dG_b1 = np.sum(dG_h1, axis=1, keepdims=True)

    # gradient descent of G
    G_W4 -= G_lr * dG_W4
    G_b4 -= G_lr * dG_b4
    G_W3 -= G_lr * dG_W3
    G_b3 -= G_lr * dG_b3
    G_W2 -= G_lr * dG_W2
    G_b2 -= G_lr * dG_b2
    G_W1 -= G_lr * dG_W1
    G_b1 -= G_lr * dG_b1

    D_losses.append(D_loss)
    G_losses.append(G_loss)

    if epoch % 500 == 0:  # show generated images every 500 epoches
        print(f"Epoch {epoch}: D_loss = {D_loss:.4f}, G_loss = {G_loss:.4f}")
        plt.figure(figsize=(15, 5))
        for i in range(3):
            z = np.random.randn(input_size, 1)
            fake_image, _, _, _ = generator(z)
            plt.subplot(1, 3, i+1)
            plt.imshow(fake_image.reshape(28, 28), cmap='gray')
            plt.title(f"z_{i}")
            plt.axis('off')
        plt.show()

plt.figure(figsize=(10,5))  # create loss graph
plt.plot(D_losses, label='discriminator loss')
plt.plot(G_losses, label='generator loss')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.title('training loss over time')
plt.legend()
plt.grid(True)
plt.show()
